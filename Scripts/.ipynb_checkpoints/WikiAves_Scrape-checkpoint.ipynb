{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiAves Data Scrape\n",
    "\n",
    "## Software needed before running the following scripts:\n",
    "1. Create a clone of the ArcGIS Pro Python Environment (See https://pro.arcgis.com/en/pro-app/arcpy/get-started/what-is-conda.htm)\n",
    "2. Activate the cloned ArcGIS Pro Python Environment in ArcGIS Pro\n",
    "3. Install **beautifulsoup4** using the Python Package Manager\n",
    "\n",
    "## Starting Jupyter Notebook from Python Command Prompt\n",
    "(arcgispro-py3-clone) C:\\Users\\whitacrej\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-clone>r:\n",
    "\n",
    "(arcgispro-py3-clone) R:\\>cd PARC\\GIS\\Projects\\PARC_Brazil\n",
    "\n",
    "(arcgispro-py3-clone) R:\\PARC\\GIS\\Projects\\PARC_Brazil>jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures as cf\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests as req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_id_lists(start, stop, records_per):\n",
    "    ''''''\n",
    "    ids_list = []\n",
    "    while start <= stop:\n",
    "        istop = start + records_per\n",
    "        if istop < stop:\n",
    "            ids_list.append(range(start, istop))\n",
    "        else:\n",
    "            ids_list.append(range(start, stop + 1))\n",
    "\n",
    "        start += records_per\n",
    "\n",
    "    return ids_list\n",
    "\n",
    "def wikiaves_scrape(i):\n",
    "    d = []\n",
    "    # Get general details\n",
    "    url = f'https://www.wikiaves.com.br/_midia_detalhes.php?m={i}'\n",
    "    r = req.get(url)\n",
    "    text = r.text\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    if not soup.findAll('div', attrs={'class':'alert alert-danger'}):\n",
    "        wa_id = [['Codigo', div.text.strip()] for div in soup.findAll('div', attrs={'class': 'wa-recordid'})]\n",
    "        wa_name = [['NomeComum', a.text.strip()] for a in soup.findAll('a', attrs={'class': 'wa-id m-link font-poppins'})]\n",
    "        wa_sp = [['Especie', t.text.strip()] for a in soup.findAll('a', attrs={'class': 'm-link'}) for t in a.findAll('i')]\n",
    "        wa_data = [t.text.strip().replace('\\xa0', ' ').split(': ') for div in soup.findAll('div', attrs={'class': 'wa-lista-detalhes'}) \n",
    "                   for t in div.findAll('div', attrs={'class': '', 'id': ''}) \n",
    "                   if t.text.strip().replace('\\xa0', ' ') != 'Local de Observação:']\n",
    "        d.extend(wa_id + wa_name + wa_sp + wa_data)\n",
    "        # Get EXIF details\n",
    "        url = f'https://www.wikiaves.com.br/getExifInfo.php?f={i}'\n",
    "        r = req.get(url)\n",
    "        text = r.text\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        if soup.findAll('div'):\n",
    "            wa_exif = [div.text.strip().replace('\\xa0', ' ').split(': ') for div in soup.findAll('div')\n",
    "                       if div.text.strip().replace('\\xa0', ' ').split(': ') not in d]\n",
    "            d.extend(wa_exif)\n",
    "    # Clean up data\n",
    "    if d:\n",
    "        d = {item[0]: item[1] for item in d if len(item) > 1}\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Records on 2020-03-20 @ 1500 EDT\n",
    "# Photos: 2,901,429\n",
    "# Sounds: 178,136\n",
    "# Total: 3,079,565\n",
    "# Last Record ID: 3725764\n",
    "\n",
    "''' Paramters '''\n",
    "start = 1 # or other number\n",
    "stop = 3754000 # or other number\n",
    "records_per = 2000 # or 5000\n",
    "\n",
    "# Output CSV File Folder\n",
    "out_csv_name = 'WikiAves_Scrape'\n",
    "out_folder = r'R:\\PARC\\GIS\\Projects\\PARC_Brazil\\RawData\\WikiAves_WebScrapes\\WikiAves_202004'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ID lists\n",
    "ids_list = create_id_lists(start, stop, records_per)\n",
    "\n",
    "print(ids_list)\n",
    "print(f'CSV Files to be created: {len(ids_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threaded WikiAves Scrape\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for ids in ids_list:\n",
    "    i_start_time = datetime.now()\n",
    "    with cf.ThreadPoolExecutor() as pool:\n",
    "        data = pool.map(wikiaves_scrape, ids)\n",
    "    \n",
    "    # Create output CSV file name and folder\n",
    "    data_frame = pd.DataFrame(filter(None, data))\n",
    "    csv_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    out_csv = os.path.join(out_folder, f'{out_csv_name}_{ids[0]}-{ids[-1]}_{csv_time}.csv')\n",
    "    data_frame.to_csv(out_csv, index=False)\n",
    "    \n",
    "    i_end_time = datetime.now()\n",
    "    i_time =str(i_end_time - i_start_time)\n",
    "                           \n",
    "    print(f'CSV Created in {i_time}: {out_csv}')\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_time = str(end_time - start_time)\n",
    "\n",
    "print(f'Total Time: {total_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-threaded code...Do Not Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-threaded (for comparison only...)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for ids in ids_list:\n",
    "    i_start_time = datetime.now()\n",
    "    data = []\n",
    "    for i in ids:\n",
    "        d = wikiaves_scrape(i)\n",
    "        data.append(d)\n",
    "    \n",
    "    # Create output CSV file name and folder\n",
    "    data_frame = pd.DataFrame(filter(None, data))\n",
    "    csv_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    out_csv = os.path.join(out_folder, f'{out_csv_name}_{ids[0]}-{ids[-1]}_{csv_time}.csv')\n",
    "    data_frame.to_csv(out_csv, index=False)\n",
    "    \n",
    "    i_end_time = datetime.now()\n",
    "    i_time =str(i_end_time - i_start_time)\n",
    "                           \n",
    "    print(f'CSV Created in {i_time}: {out_csv}')\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_time = str(end_time - start_time)\n",
    "\n",
    "print(f'Total Time: {total_time}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
